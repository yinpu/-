{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MFFRM(without KGE).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1sGM-AYRVwll8UyXRa1K5lwjflrKLpiVZ",
      "authorship_tag": "ABX9TyMwx2VT3BbaBO7wKGrCVMpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinpu/deep_learning_study/blob/master/MFFRM(without_KGE).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkXKXGOpoLp9",
        "colab_type": "text"
      },
      "source": [
        "# MFFRM(without KGE)\n",
        "**去掉知识图谱部分**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j7_lNqESFW3",
        "colab_type": "text"
      },
      "source": [
        "## 数据预处理\n",
        "* book -->book-crossing数据集\n",
        "* music -->last.fm数据集\n",
        "* movie -->movielen数据集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iZ8QRG2HpWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#数据预处理函数\n",
        "import numpy as np\n",
        "import os\n",
        "import  tensorflow as tf\n",
        "\n",
        "RATING_FILE_NAME={\n",
        "    'movie': 'ratings.dat',\n",
        "    'book':'BX-Book-Ratings.csv',\n",
        "    'music':'user_artists.dat'\n",
        "}\n",
        "SEP={\n",
        "    'movie':'::',\n",
        "    'book':';',\n",
        "    'music':'\\t'\n",
        "}\n",
        "THRESHOLD={\n",
        "    'movie':4,\n",
        "    'book':0,\n",
        "    'music':0\n",
        "}\n",
        "DATASETS={'movie','book','music'}\n",
        "\n",
        "def read_item_index2entity_id_file():\n",
        "    file = './data/'+DATASET+'/item_index2entity_id.txt'\n",
        "    i=0\n",
        "    for line in open(file,encoding='utf-8').readlines():\n",
        "        item_index=line.strip().split('\\t')[0]\n",
        "        satori_id=line.strip().split('\\t')[1]\n",
        "        item_index_old2new[item_index]=i\n",
        "        entity_id2index[satori_id]=i\n",
        "        i+=1\n",
        "        \n",
        "def convert_rating():\n",
        "    file = './data/'+DATASET+'/'+RATING_FILE_NAME[DATASET]\n",
        "    item_set=set(item_index_old2new.values())\n",
        "    user_pos_rating={}\n",
        "    user_neg_rating={}\n",
        "    for line in open(file,encoding='utf-8').readlines():\n",
        "        array =line.strip().split(SEP[DATASET])\n",
        "        if DATASET=='book':\n",
        "            array=list(map(lambda x:x[1:-1],array))\n",
        "        item_index_old=array[1]\n",
        "        if item_index_old not in item_index_old2new:\n",
        "            continue\n",
        "        item_index=item_index_old2new[item_index_old]\n",
        "        user_index_old=int(array[0])\n",
        "        rating=float(array[2])\n",
        "        if rating >=THRESHOLD[DATASET]:\n",
        "            if user_index_old not in user_pos_rating:\n",
        "                user_pos_rating[user_index_old]=set()\n",
        "            user_pos_rating[user_index_old].add(item_index)\n",
        "        else:\n",
        "            if user_index_old not in user_neg_rating:\n",
        "                user_neg_rating[user_index_old]=set()\n",
        "            user_neg_rating[user_index_old].add(item_index)\n",
        "    print('coverting rating file . . .')\n",
        "    writer=open('data/'+DATASET+'/rating_final.txt','w',encoding='utf-8')\n",
        "    user_cnt=0\n",
        "    user_index_old2new={}\n",
        "    for user_index_old,pos_item_set in user_pos_rating.items():\n",
        "        if user_index_old not in user_index_old2new:\n",
        "            user_index_old2new[user_index_old]=user_cnt\n",
        "            user_cnt+=1\n",
        "        user_index=user_index_old2new[user_index_old]\n",
        "        for item in pos_item_set:\n",
        "            writer.write(\"%d\\t%d\\t1\\n\"%(user_index,item))\n",
        "        unwatch_set=item_set-pos_item_set\n",
        "        if user_index_old in user_neg_rating:\n",
        "            unwatch_set-=user_neg_rating[user_index_old]\n",
        "        for item in np.random.choice(list(unwatch_set),size=len(pos_item_set),replace=False):\n",
        "            writer.write(\"%d\\t%d\\t0\\n\"%(user_index,item))\n",
        "    writer.close()\n",
        "    print(\"num of users:\",user_cnt)\n",
        "    print(\"num of item:\",len(item_set))\n",
        "\n",
        "def convert_kg():\n",
        "    print('converting kg.txt file')\n",
        "    entity_cnt=len(entity_id2index)\n",
        "    relation_cnt=0\n",
        "    writer=open('data/'+DATASET+'/kg_final.txt','w',encoding='utf-8')\n",
        "    file=open('data/'+DATASET+'/kg.txt','r',encoding='utf-8')\n",
        "    for line in file:\n",
        "        array=line.strip().split('\\t')\n",
        "        head_old=array[0]\n",
        "        relation_old=array[1]\n",
        "        tail_old=array[2]\n",
        "        if head_old not in entity_id2index:\n",
        "            continue\n",
        "        head=entity_id2index[head_old]\n",
        "        if tail_old not in entity_id2index:\n",
        "            entity_id2index[tail_old]=entity_cnt\n",
        "            entity_cnt+=1\n",
        "        tail=entity_id2index[tail_old]\n",
        "        if relation_old not in relation_id2index:\n",
        "            relation_id2index[relation_old]=relation_cnt\n",
        "            relation_cnt+=1\n",
        "        relation=relation_id2index[relation_old]\n",
        "        writer.write(\"%d\\t%d\\t%d\\n\"%(head,relation,tail))\n",
        "    writer.close()\n",
        "    print('num of entities:',entity_cnt)\n",
        "    print('num of relations:',relation_cnt)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG8I-NuZSFYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e479559e-9d7c-4c9e-b7fc-04cf527ac548"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir('/content/drive/Shared drives/yinpu/MyProject/FFMKR')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aXf42_KSFW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "f82fb5a8-3b00-4596-8dff-a23cbab881c5"
      },
      "source": [
        "#数据集处理\n",
        "for DATASET in DATASETS:\n",
        "    print(DATASET+\"  :   \")\n",
        "    np.random.seed(555)\n",
        "    entity_id2index={}\n",
        "    relation_id2index={}\n",
        "    item_index_old2new={}\n",
        "    read_item_index2entity_id_file()\n",
        "    convert_rating()\n",
        "    convert_kg()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movie  :   \n",
            "coverting rating file . . .\n",
            "num of users: 6036\n",
            "num of item: 2347\n",
            "converting kg.txt file\n",
            "num of entities: 6729\n",
            "num of relations: 7\n",
            "music  :   \n",
            "coverting rating file . . .\n",
            "num of users: 1872\n",
            "num of item: 3846\n",
            "converting kg.txt file\n",
            "num of entities: 9366\n",
            "num of relations: 60\n",
            "book  :   \n",
            "coverting rating file . . .\n",
            "num of users: 17860\n",
            "num of item: 14910\n",
            "converting kg.txt file\n",
            "num of entities: 24039\n",
            "num of relations: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbYd8WmNptxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(n_user: int, n_item: int, dim=8, L=1,H=1, l2=1e-6) -> tf.keras.Model:\n",
        "    l2=tf.keras.regularizers.l2(l2)\n",
        "    user_id=tf.keras.Input(shape=(),dtype=tf.int32,name='user_id')\n",
        "    item_id=tf.keras.Input(shape=(),dtype=tf.int32,name='item_id')\n",
        "    # head_id=tf.keras.Input(shape=(),dtype=tf.int32,name='head_id')\n",
        "    # relation_id=tf.keras.Input(shape=(),dtype=tf.int32,name='relation_id')\n",
        "    # tail_id=tf.keras.Input(shape=(),dtype=tf.int32,name='tail_id')\n",
        "    user_embedding=tf.keras.layers.Embedding(input_dim=n_user,output_dim=dim,\n",
        "                                             embeddings_regularizer=l2)\n",
        "    item_embedding=tf.keras.layers.Embedding(input_dim=n_item,output_dim=dim,\n",
        "                                             embeddings_regularizer=l2)\n",
        "    # entity_embedding=tf.keras.layers.Embedding(input_dim=n_entity,output_dim=dim,\n",
        "    #                                            embeddings_regularizer=l2)\n",
        "    # relation_embedding=tf.keras.layers.Embedding(input_dim=n_relation,output_dim=dim,\n",
        "    #                                            embeddings_regularizer=l2)\n",
        "    u=user_embedding(user_id)\n",
        "    i=item_embedding(item_id)\n",
        "    # h=entity_embedding(head_id)\n",
        "    # t=entity_embedding(tail_id)\n",
        "    # r=relation_embedding(relation_id)\n",
        "    for _ in range(L):\n",
        "        u=tf.keras.layers.Dense(dim,activation=tf.keras.activations.relu,kernel_regularizer=l2)(u)\n",
        "        i=tf.keras.layers.Dense(dim,activation=tf.keras.activations.relu,kernel_regularizer=l2)(i)\n",
        "        # i,h=feature_fusion_unit(inputs=(i,h),weight_regularizer=l2)\n",
        "        # t=tf.keras.layers.Dense(dim,activation=tf.keras.activations.relu,kernel_regularizer=l2)(t)\n",
        "    rs=tf.keras.activations.relu(u*i)\n",
        "    for _ in range(H-1):\n",
        "        rs=tf.keras.layers.Dense(dim,activation=tf.keras.activations.relu,kernel_regularizer=l2)(rs)\n",
        "    rs=tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid,kernel_regularizer=l2)(rs)\n",
        "    return tf.keras.Model(inputs=[user_id,item_id,head_id],outputs=rs)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCc5BEl9qMGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(DATASET):\n",
        "    n_user,n_item,train_data,eval_data,test_data=load_rating(DATASET)\n",
        "    n_entity,n_relation,kg=load_kg(DATASET)\n",
        "    print('data loaded')\n",
        "    return n_user,n_item,n_entity,n_relation,train_data,eval_data,test_data,kg\n",
        "\n",
        "def load_rating(DATASET):\n",
        "    print('reading rating file ....')\n",
        "    rating_file='data/'+DATASET+'/rating_final'\n",
        "    if os.path.exists(rating_file+'.npy'):\n",
        "        rating_np=np.load(rating_file+'.npy')\n",
        "    else:\n",
        "        rating_np=np.loadtxt(rating_file+'.txt',dtype=np.int32)\n",
        "        np.save(rating_file+'.npy',rating_np)\n",
        "    n_user=len(set(rating_np[:,0]))\n",
        "    n_item=len(set(rating_np[:,1]))\n",
        "    train_data,eval_data,test_data=dataset_split(rating_np)\n",
        "    return n_user,n_item,train_data,eval_data,test_data\n",
        "\n",
        "def dataset_split(rating_np):\n",
        "    print('splitting dataset ...')\n",
        "    eval_ratio=0.2\n",
        "    test_ration=0.2\n",
        "    n_rating=rating_np.shape[0]\n",
        "    eval_indices=np.random.choice(list(range(n_rating)),size=int(n_rating*eval_ratio),replace=False)\n",
        "    left=set(range(n_rating))-set(eval_indices)\n",
        "    test_indices=np.random.choice(list(left),size=int(n_rating*test_ration),replace=False)\n",
        "    train_indices=list(left-set(test_indices))\n",
        "    train_data,eval_data,test_data=rating_np[train_indices],rating_np[eval_indices],rating_np[test_indices]\n",
        "    return train_data,eval_data,test_data\n",
        "def load_kg(DATASET):\n",
        "    print('reading KG file')\n",
        "    kg_file='data/'+DATASET+'/kg_final'\n",
        "    if os.path.exists(kg_file + '.npy'):\n",
        "        kg = np.load(kg_file + '.npy')\n",
        "    else:\n",
        "        kg = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n",
        "        np.save(kg_file + '.npy', kg)\n",
        "    n_entity=len(set(kg[:,0])|set(kg[:,2]))\n",
        "    n_relation=len(set(kg[:,1]))\n",
        "    return n_entity,n_relation,kg"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7C2LNipqR7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(data):\n",
        "    u = tf.constant(data[:, 0], dtype=tf.int32)\n",
        "    i = tf.constant(data[:, 1], dtype=tf.int32)\n",
        "    h = tf.constant(data[:, 1], dtype=tf.int32)\n",
        "    r = tf.constant(data[:, 2], dtype=tf.float32)\n",
        "    return {'user_id': u, 'item_id': i}, r\n",
        "\n",
        "def train(DATASET, DEFAULT):\n",
        "    n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg =load_data(DATASET)\n",
        "    rs_model = model(n_user, n_item,dim=DEFAULT[DATASET]['dim'])\n",
        "    epochs = DEFAULT[DATASET]['epochs']\n",
        "    batch = DEFAULT[DATASET]['batch_size']\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices(preprocess_data(train_data))\n",
        "    train_ds = train_ds.shuffle(len(train_data)).batch(batch)\n",
        "    eval_ds = tf.data.Dataset.from_tensor_slices(preprocess_data(eval_data))\n",
        "    eval_ds =eval_ds.batch(batch)\n",
        "    rs_model.compile(optimizer=tf.optimizers.Adam( DEFAULT[DATASET]['lr_rs']), loss=tf.keras.losses.binary_crossentropy,\n",
        "                     metrics=['AUC', tf.keras.metrics.binary_accuracy])\n",
        "    for epoch in range(epochs):\n",
        "        history=rs_model.fit(train_ds, epochs=epoch + 1, validation_data=eval_ds, verbose=1, initial_epoch=epoch)\n",
        "    return rs_model"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mc2DMTUqVt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "303aa3d6-4613-4ea3-d1e5-c235642a0350"
      },
      "source": [
        "DEFAULT={\n",
        "        'movie': {'epochs': 10, 'dim': 8, 'L': 1, 'H': 1, 'batch_size': 4096, 'lr_rs': 0.02, 'lr_kg': 0.01,\n",
        "                  'kge_interval': 3},\n",
        "        'book': {'epochs': 10, 'dim': 8, 'L': 1, 'H': 1, 'batch_size': 32, 'lr_rs': 2e-4, 'lr_kg': 2e-5,\n",
        "                 'kge_interval': 2},\n",
        "        'music': {'epochs': 30, 'dim': 4, 'L': 2, 'H': 1, 'batch_size': 256, 'lr_rs': 1e-4, 'lr_kg': 2e-4,\n",
        "                  'kge_interval': 2}\n",
        "    }\n",
        "DATASET='movie'\n",
        "rs_model=train(DATASET,DEFAULT)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading rating file ....\n",
            "splitting dataset ...\n",
            "reading KG file\n",
            "data loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "111/111 [==============================] - 2s 20ms/step - loss: 0.4538 - auc: 0.8686 - binary_accuracy: 0.7915 - val_loss: 0.4257 - val_auc: 0.8851 - val_binary_accuracy: 0.8032\n",
            "Epoch 2/2\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.4155 - auc: 0.8906 - binary_accuracy: 0.8077 - val_loss: 0.4260 - val_auc: 0.8860 - val_binary_accuracy: 0.8034\n",
            "Epoch 3/3\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.4056 - auc: 0.8969 - binary_accuracy: 0.8144 - val_loss: 0.4257 - val_auc: 0.8876 - val_binary_accuracy: 0.8075\n",
            "Epoch 4/4\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.3868 - auc: 0.9086 - binary_accuracy: 0.8269 - val_loss: 0.4111 - val_auc: 0.8980 - val_binary_accuracy: 0.8192\n",
            "Epoch 5/5\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.3591 - auc: 0.9238 - binary_accuracy: 0.8446 - val_loss: 0.3998 - val_auc: 0.9063 - val_binary_accuracy: 0.8271\n",
            "Epoch 6/6\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.3381 - auc: 0.9343 - binary_accuracy: 0.8574 - val_loss: 0.3959 - val_auc: 0.9101 - val_binary_accuracy: 0.8330\n",
            "Epoch 7/7\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.3247 - auc: 0.9407 - binary_accuracy: 0.8656 - val_loss: 0.4008 - val_auc: 0.9115 - val_binary_accuracy: 0.8349\n",
            "Epoch 8/8\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.3153 - auc: 0.9450 - binary_accuracy: 0.8711 - val_loss: 0.4004 - val_auc: 0.9126 - val_binary_accuracy: 0.8353\n",
            "Epoch 9/9\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.3080 - auc: 0.9483 - binary_accuracy: 0.8753 - val_loss: 0.4059 - val_auc: 0.9125 - val_binary_accuracy: 0.8363\n",
            "Epoch 10/10\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 0.3031 - auc: 0.9505 - binary_accuracy: 0.8782 - val_loss: 0.4064 - val_auc: 0.9116 - val_binary_accuracy: 0.8343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSZaZJAOqjV1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "a2f24794-647e-425e-ab74-50186a7bdcc3"
      },
      "source": [
        "n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg =load_data(DATASET)\n",
        "result=rs_model.evaluate([test_data[:,0],test_data[:,1]],test_data[:,2])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading rating file ....\n",
            "splitting dataset ...\n",
            "reading KG file\n",
            "data loaded\n",
            "4712/4712 [==============================] - 17s 4ms/step - loss: 0.3264 - auc: 0.9425 - binary_accuracy: 0.8720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJ75q-MsNwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b2d74376-1d0a-4deb-c85c-7bf7c70e3140"
      },
      "source": [
        "print(\"在测试集上auc：\"+str(result[1]))\n",
        "print(\"在测试集上acc：\"+str(result[2]))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "在测试集上auc：0.9425419569015503\n",
            "在测试集上acc：0.8720232844352722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHYYuvsutmAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "1b40181f-d8fc-4dba-f4e2-4d377839d422"
      },
      "source": [
        "DEFAULT={\n",
        "        'movie': {'epochs': 10, 'dim': 8, 'L': 1, 'H': 1, 'batch_size': 4096, 'lr_rs': 0.02, 'lr_kg': 0.01,\n",
        "                  'kge_interval': 3},\n",
        "        'book': {'epochs': 2, 'dim': 8, 'L': 1, 'H': 1, 'batch_size': 32, 'lr_rs': 0.001, 'lr_kg': 2e-5,\n",
        "                 'kge_interval': 2},\n",
        "        'music': {'epochs': 30, 'dim': 4, 'L': 2, 'H': 1, 'batch_size': 256, 'lr_rs': 1e-4, 'lr_kg': 2e-4,\n",
        "                  'kge_interval': 2}\n",
        "    }\n",
        "DATASET='book'\n",
        "rs_model=train(DATASET,DEFAULT)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading rating file ....\n",
            "splitting dataset ...\n",
            "reading KG file\n",
            "data loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2621/2621 [==============================] - 19s 7ms/step - loss: 0.6065 - auc: 0.7029 - binary_accuracy: 0.6604 - val_loss: 0.5809 - val_auc: 0.7329 - val_binary_accuracy: 0.6984\n",
            "Epoch 2/2\n",
            "2621/2621 [==============================] - 19s 7ms/step - loss: 0.5191 - auc: 0.8153 - binary_accuracy: 0.7394 - val_loss: 0.5980 - val_auc: 0.7360 - val_binary_accuracy: 0.6920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt1i7qoFtx9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d57e45ad-6551-4dc7-a971-90cbc21eaa46"
      },
      "source": [
        "n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg =load_data(DATASET)\n",
        "result=rs_model.evaluate([test_data[:,0],test_data[:,1]],test_data[:,2])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading rating file ....\n",
            "splitting dataset ...\n",
            "reading KG file\n",
            "data loaded\n",
            "874/874 [==============================] - 3s 4ms/step - loss: 0.4989 - auc: 0.8409 - binary_accuracy: 0.7608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KqF5fgpyUMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1d9d6f7d-3ae6-4399-d5c8-9dd304ff0324"
      },
      "source": [
        "print(\"在测试集上auc：\"+str(result[1]))\n",
        "print(\"在测试集上acc：\"+str(result[2]))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "在测试集上auc：0.8408634066581726\n",
            "在测试集上acc：0.760778546333313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i78A0jrsyfMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "47d17a19-7d69-49fe-9e68-8e23fc0c693c"
      },
      "source": [
        "DEFAULT={\n",
        "        'movie': {'epochs': 10, 'dim': 8, 'L': 1, 'H': 1, 'batch_size': 4096, 'lr_rs': 0.02, 'lr_kg': 0.01,\n",
        "                  'kge_interval': 3},\n",
        "        'book': {'epochs': 3, 'dim': 8, 'L': 1, 'H': 1, 'batch_size': 32, 'lr_rs': 0.001, 'lr_kg': 2e-5,\n",
        "                 'kge_interval': 2},\n",
        "        'music': {'epochs':4, 'dim': 8, 'L': 1, 'H': 1, 'batch_size': 256, 'lr_rs': 1e-3, 'lr_kg': 2e-4,\n",
        "                  'kge_interval': 2}\n",
        "    }\n",
        "DATASET='music'\n",
        "rs_model=train(DATASET,DEFAULT)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading rating file ....\n",
            "splitting dataset ...\n",
            "reading KG file\n",
            "data loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 1s 10ms/step - loss: 0.6718 - auc: 0.6807 - binary_accuracy: 0.6254 - val_loss: 0.6054 - val_auc: 0.7725 - val_binary_accuracy: 0.7272\n",
            "Epoch 2/2\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.5211 - auc: 0.8158 - binary_accuracy: 0.7599 - val_loss: 0.5282 - val_auc: 0.7969 - val_binary_accuracy: 0.7492\n",
            "Epoch 3/3\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.4557 - auc: 0.8674 - binary_accuracy: 0.7906 - val_loss: 0.5482 - val_auc: 0.7971 - val_binary_accuracy: 0.7422\n",
            "Epoch 4/4\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.4236 - auc: 0.8877 - binary_accuracy: 0.8066 - val_loss: 0.5726 - val_auc: 0.7944 - val_binary_accuracy: 0.7254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oBJDU4WymSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b7a99ba0-d55b-4f9e-da2e-4d59d778ea62"
      },
      "source": [
        "n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg =load_data(DATASET)\n",
        "result=rs_model.evaluate([test_data[:,0],test_data[:,1]],test_data[:,2])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading rating file ....\n",
            "splitting dataset ...\n",
            "reading KG file\n",
            "data loaded\n",
            "265/265 [==============================] - 1s 4ms/step - loss: 0.4571 - auc: 0.8670 - binary_accuracy: 0.7941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unb-2dEfzNoo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7e345298-0740-49ae-b8fd-56105c26b0b7"
      },
      "source": [
        "print(\"在测试集上auc：\"+str(result[1]))\n",
        "print(\"在测试集上acc：\"+str(result[2]))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "在测试集上auc：0.8670268654823303\n",
            "在测试集上acc：0.7940725088119507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBWfSefW0vW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}